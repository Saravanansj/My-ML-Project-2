**Provide Better Diabetes Treatment using Reinforcement Learning**

# ‚ö† WARNING: Educational Purpose Only

# ‚ö†Ô∏è **IMPORTANT WARNING**

## **This software is provided strictly for educational and research purposes only.**  
## **It must NOT be used as a substitute for:**  
- **Professional medical advice**  
- **Diagnosis**  
- **Treatment**  
- **Safety-critical decision making**


## Disclaimer
The solutions and implementations in this project are provided based on my knowledge and online references.  
They may not be fully correct or complete, so please **review and verify them before use**.  
Feedback, suggestions, reports of mistakes, and ideas for improvement are always welcome to help enhance the project.


## Introduction

This project demonstrates the application of Reinforcement Learning (RL) algorithms ‚Äî Q-Learning and SARSA ‚Äî to optimize diabetes treatment decisions. By modeling patient health states and treatment actions as a Markov Decision Process (MDP), the system learns an optimal treatment strategy to improve patient outcomes.

The model trains an agent to decide whether to recommend Insulin, Oral Medication, Diet Adjustment, or Exercise, depending on whether the patient‚Äôs diabetes is Controlled or Uncontrolled.

## Abstract 

The project implements Q-Learning (off-policy) and SARSA (on-policy) algorithms to train an agent that recommends diabetes treatments.

State Space: Controlled, Uncontrolled

Action Space: Insulin, Oral Medication, Diet Adjustment, Exercise

Q-Table: Stores expected cumulative rewards for each state-action pair.

Reward Structure: Reflects the effectiveness of treatments (positive for effective, negative/low for ineffective).

The system allows the user to select an algorithm, train the model, view the learned Q-table, and receive treatment recommendations.

**Reinforcement Learning Concepts**

Agent ‚Üí Learns and makes treatment decisions

Environment ‚Üí Patient health condition

State (s) ‚Üí Controlled / Uncontrolled

Action (a) ‚Üí Treatment options

Reward (R) ‚Üí Effectiveness of treatment

Policy (œÄ) ‚Üí Strategy mapping states ‚Üí actions

Q-Value ‚Üí Action-value function estimating rewards for each state-action pair

Exploration vs Exploitation ‚Üí Epsilon-greedy strategy to balance new discoveries and known best actions

Discount Factor (Œ≥) ‚Üí Importance of future rewards

## Algorithms Implemented
**Q-Learning (Off-Policy)**

Updates Q-values using the maximum future reward, regardless of actions actually taken.

Faster convergence, but can be unstable.

Update rule:

Q(s,a) ‚Üê Q(s,a) + Œ± [ R + Œ≥ max_a' Q(s‚Äô,a‚Äô) ‚Äì Q(s,a) ]

**SARSA (On-Policy)**

Updates Q-values based on the action actually taken under the current policy.

Slower but more stable.

Update rule:

Q(s,a) ‚Üê Q(s,a) + Œ± [ R + Œ≥ Q(s‚Äô,a‚Äô) ‚Äì Q(s,a) ]

**Algorithm Implementation**

Q-Table Initialization ‚Üí 2D array with zeros

Hyperparameters ‚Üí learning rate, discount factor, epsilon (exploration rate), episodes

Reward Function ‚Üí Positive for effective treatments (e.g., Insulin in Controlled state = +10), negative/low for ineffective ones

Transition Function ‚Üí Defines how patient states change based on actions (e.g., Insulin and Exercise more likely to maintain Controlled state)

Action Selection ‚Üí Epsilon-greedy policy

**Prediction Comparison**
Aspect	Q-Learning	SARSA
Convergence	Faster, off-policy	Slower, on-policy
Policy Outcome	Aggressive, favors max reward	Conservative, reflects actual actions
Exploration	Strong, prioritizes best possible reward	Cautious, depends on current policy
Stability	Less stable, may oscillate	More stable, smoother adjustments
Example Output	Suggested Exercise for Controlled	Suggested Insulin for Controlled


**Sample Outputs**

Q-Learning Example:

Learned Q-table:
[[58.97 44.23 52.85 70.  ]
 [39.64 55.6  57.6  64.  ]]

Suggested treatment for 'Controlled': Exercise


SARSA Example:

Learned Q-table:
[[83.11 62.10 61.01 75.33]
 [39.59 47.42 50.04 75.00]]

Suggested treatment for 'Controlled': Insulin

**Conclusion**

Q-Learning ‚Üí Aggressive, fast optimization, favors maximizing long-term rewards. In this setup, often suggested Exercise for controlled patients.

SARSA ‚Üí More conservative, experience-driven, ensures stability. Frequently suggested Insulin for controlled patients.

üëâ Q-Learning is ideal for quick optimization.
üëâ SARSA ensures stable, experience-based decisions.

**License**

This project is licensed under the MIT License.

**Acknowledgments**

Project developed as part of academic work at Government College of Engineering, Bodinayakanur(In TVS training and Services).

Inspired by Sutton & Barto‚Äôs Reinforcement Learning textbook.

**Disclaimer**

**Built for educational purposes only ‚Äî not a substitute for professional medical advice.**
